{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Callable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from Bio import SeqIO\n",
    "from Bio.pairwise2 import align\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.logs import logger, delete_all_runs, tb_run, reinit_tensorboard_local\n",
    "from src.metrics import f1_score, precision_score, matthews_corrcoef, recall_score, accuracy_score\n",
    "from src.models import HybridModel, SimpleCNN\n",
    "from src.parameters import ModelParameters, Hyperparameters\n",
    "from src.processing import SequenceAugmentator, OneHotEncoder, get_single_seq_patches\n",
    "from src.training import Trainer\n",
    "from src.validation import nested_cv, NestedCVresults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-J0rHGGISn8"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrkhIyJAZOqf"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "OTHER_PROTEINS_PATH = os.path.join(DATA_PATH, \"other_proteins.fasta\")\n",
    "NOT_APIDAECINS_PATH = os.path.join(DATA_PATH, \"not_apidaecins.fasta\")\n",
    "APIDAECINS_PATH = os.path.join(DATA_PATH, \"apidaecins.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1-BwDVoYurk",
    "outputId": "1387f557-3232-48f1-cecd-8803a52fc7d5"
   },
   "outputs": [],
   "source": [
    "# Load all apidaecin sequences (includes unfiltered peptides that do not belong to the group)\n",
    "apidaecins_sequences = list(set(map(lambda rec: str(rec.seq), SeqIO.parse(APIDAECINS_PATH, \"fasta\"))))\n",
    "len(apidaecins_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIkQ7o-pbEk0",
    "outputId": "eab3fc0f-1e36-4959-94c4-e8d118741612"
   },
   "outputs": [],
   "source": [
    "# Load protein non-AMP sequences\n",
    "other_proteins_sequences = list(set(map(lambda rec: str(rec.seq), SeqIO.parse(OTHER_PROTEINS_PATH, \"fasta\"))))\n",
    "len(other_proteins_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load antipositive sequences\n",
    "antipos_sequences = set()\n",
    "for record in SeqIO.parse(NOT_APIDAECINS_PATH, \"fasta\"):\n",
    "    if \"Apidaecin\" in record.description or \"apidaecin\" in record.description:\n",
    "        continue\n",
    "    antipos_sequences.add(str(record.seq))\n",
    "antipos_sequences = list(antipos_sequences)\n",
    "len(antipos_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-5QsqsJA-gi"
   },
   "source": [
    "# Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5nvZpYQHwkp"
   },
   "outputs": [],
   "source": [
    "# Pro-apidaecins are filtered apidaecins without signal peptides\n",
    "PRO_APIDAECINS_PATH = os.path.join(DATA_PATH, \"pro-apidaecins.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEsXSFAPCvKX"
   },
   "outputs": [],
   "source": [
    "# Signal peptides are cut based on alignment position of first linker\n",
    "linkers = [\"RREAKPEAEP\", \"RREAEPEAEP\", \"RREAEPDP\", \"RREAEPDP\", \"RREAGPEPEP\", \"RREPDPEP\", \"REAKPEPEPEP\", \"RREPDPEP\", \"RREAEPDP\", \"EADPAKP\", \"QAEPGKP\"]\n",
    "\n",
    "approved_records = []\n",
    "\n",
    "approved_seqs = set()\n",
    "for record in SeqIO.parse(APIDAECINS_PATH, \"fasta\"):\n",
    "    if \"PREDICTED\" not in record.description:\n",
    "        seq_alignments = []\n",
    "        for linker in linkers:\n",
    "            alignments = align.localxs(record.seq, linker, -11, -1)\n",
    "            first_linker_algn = min(alignments, key=lambda x: x.start)\n",
    "            seq_alignments.append(first_linker_algn)\n",
    "        max_score = max(seq_alignments, key=lambda x: x.score).score\n",
    "        best_alignments = list(filter(lambda x: x.score == max_score, seq_alignments))\n",
    "        best_alignment = min(best_alignments, key=lambda x: x.start)\n",
    "        if 80 > best_alignment.start > 12 and best_alignment.score > 6:\n",
    "            new_seq = record.seq[best_alignment.start:]\n",
    "            if new_seq not in approved_seqs:\n",
    "                record.seq = new_seq\n",
    "                approved_records.append(record)\n",
    "            approved_seqs.add(new_seq)\n",
    "\n",
    "# Uncomment line below to save filtered sequences to file\n",
    "# SeqIO.write(approved_records, PRO_APIDAECINS_PATH, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orshuNDLE5L4",
    "outputId": "611667d4-b859-4d67-b800-d8f00b18eb5f"
   },
   "outputs": [],
   "source": [
    "# Load filtered apidaecins\n",
    "pro_apidaecins_sequences = list(set(map(lambda rec: str(rec.seq), SeqIO.parse(PRO_APIDAECINS_PATH, \"fasta\"))))\n",
    "len(pro_apidaecins_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take a look on sequences of interest\n",
    "for record in SeqIO.parse(PRO_APIDAECINS_PATH, \"fasta\"):\n",
    "    print(record.id, record.seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXg8lhAWBDQQ"
   },
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUBC6S1ZwNjM"
   },
   "outputs": [],
   "source": [
    "# Set up augmentation\n",
    "augmentator = SequenceAugmentator(\"BLOSUM45\", replacement_proba_factor=50)\n",
    "patches = get_single_seq_patches(pro_apidaecins_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YTaQ-IeDiQI",
    "outputId": "39757d5e-d8ca-4d98-dbda-87d0b9569db5"
   },
   "outputs": [],
   "source": [
    "# Test augmentation on small portion of data. 'False' means augmentation occured\n",
    "for patch in patches[:10]:\n",
    "    new_str = augmentator.apply_augmentation(patch)\n",
    "    print(patch, new_str, patch==new_str, sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yj8rUaHkdCw"
   },
   "outputs": [],
   "source": [
    "# Here you can set up adjustable training and model parameters \n",
    "torch.manual_seed(42)\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "mp = ModelParameters(\n",
    "    n_classes=2,\n",
    "    embedding_size=20,\n",
    "    conv_channels=64,\n",
    "    conv_kernel_size=13,\n",
    "    dropout_rate=0.2,\n",
    "\n",
    "    blstm_output_size=128,\n",
    "    lstm_output_size=128,\n",
    "\n",
    "    activation=\"relu\"\n",
    ")\n",
    "\n",
    "hp = Hyperparameters(\n",
    "        device=DEVICE,\n",
    "        batch_size=1000,\n",
    "        patch_size=50,\n",
    "        patch_stride=1,\n",
    "        substitution_matrix=\"BLOSUM45\",\n",
    "        replacement_proba_factor=250,\n",
    "        pos_proba=0.1,\n",
    "        antipos_proba=0.1,\n",
    "\n",
    "        model_parameters=mp,\n",
    "        encoder=OneHotEncoder(alphabet=\"prot\", device=DEVICE),\n",
    "\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        optimizer=\"adam\",\n",
    "        scheduler=None,\n",
    "        lr=1e-4,\n",
    "\n",
    "        metric_fns=(matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0xV7oyaLYj3"
   },
   "outputs": [],
   "source": [
    "# Train-test split of data\n",
    "raw_X = np.array(pro_apidaecins_sequences + other_proteins_sequences + antipos_sequences)\n",
    "pos_labels = [1] * len(pro_apidaecins_sequences)\n",
    "neg_labels = [0] * len(other_proteins_sequences) + [2] * len(antipos_sequences)\n",
    "raw_y = np.array(pos_labels + neg_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_X, raw_y, random_state=42, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(HybridModel, X_train, X_test,\n",
    "                  y_train, y_test, hp, setup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinit_tensorboard_local(tb_run(\"Model2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGZRwmekUFAR",
    "outputId": "eea7823d-2ce9-48f2-c2e5-743c97baea42",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "visualize_sequences = [\"RREAEPEAEPGNNRPVYIPPPRPPHPRLRREAEPEAEPGNNRPVYIPQPRPPHPRTAVALVSASRSFFSFAVPRALEQQRFRYSHATKR\",\n",
    "                       \"RRIRPRPPRLPRPRPRPLPFPRPGPRPIPRPLPFPRPGPRPIPRPLPFPRPGPRPIPRPL\",\n",
    "                       \"MKKIYVAGGCFWGVQGFLKTIKGIKKTTVGYANSLLENPTYELVKSHVTDAVETVEVIYDENILSLKDIVKKLFAVIDPTARNYQGPDHGRQYRNGFYFVDQEDGVMLRELMLEFSKKYEKPLATEILPLDNYYLAEDYHQDYFDKHPNAVCHIKF\"]\n",
    "\n",
    "writer = SummaryWriter(tb_run(\"Model2\"))\n",
    "trainer.train(n_epochs=100, valid=1, writer=writer, vis_seqs=visualize_sequences, cache_embeddings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate on anti-positive peptides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below shows antipositive peptides, whose fragments were predicted as apidaecin-like AMP. In the best case there should not be proline rich AMP, that are not apidaecins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for record in SeqIO.parse(NOT_APIDAECINS_PATH, \"fasta\"):\n",
    "    mask = trainer.predict_mask(str(record.seq))\n",
    "    if sum(mask) > 0:\n",
    "        print(record.id)\n",
    "        print(record.seq)\n",
    "        print(*mask, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yejxhxAN6q-E"
   },
   "source": [
    "# Hyperparameter optimization & Nested CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up objective function that executes code for single optuna trial. This includes parameters suggesting and cross-validation.\n",
    "\n",
    "You can manually adjust suggested parameters and their boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSLl7PJYFrbg"
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial, trainer_preset: Callable, X: List[str], y: List[int],\n",
    "              kfold: StratifiedKFold, mp_preset: Callable, hp_preset: Callable, outer_split: int, use_tensorboard: bool = False):\n",
    "    \"\"\"\n",
    "    Function to maximize precision by optuna\n",
    "    \n",
    "    :param trial: Optuna single trial\n",
    "    :type trial: optuna.Trial\n",
    "    :param trainer_preset: Trainer class with preset arguments\n",
    "    :type trainer_preset: Callable\n",
    "    :param X: A list of sequences of any length\n",
    "    :type X: List[str]\n",
    "    :param y: A list of labels corresponding to each sequence in X\n",
    "    :type y: List[int]\n",
    "    :param kfold: Sklearn Kfold object producing splits for data\n",
    "    :type kfold: StratifiedKfold\n",
    "    :param mp_preset: ModelParameters class with preset parameters\n",
    "    :type mp_preset: Callable\n",
    "    :param hp_preset: Hyperparameters class with preset parameters\n",
    "    :type hp_preset: Callable\n",
    "    :param outer_split: Current outer split\n",
    "    :type outer split: int\n",
    "    :param use_tensorboard: Whether to use Tensorboard for logging\n",
    "    :type use_tensorboard: bool\n",
    "    \"\"\"\n",
    "    logger.info(f\"Running nested CV, outer split {outer_split[0]}/{outer_split[1]} trial #{trial.number}\")\n",
    "\n",
    "    patch_size = trial.suggest_int(\"patch_size\", 20, 60)\n",
    "    patch_stride = trial.suggest_int(\"patch_stride\", 1, patch_size, log=True)\n",
    "    substitution_matrix = trial.suggest_categorical(\"substitution_matrix\", [\"BLOSUM45\", \"BLOSUM62\", \"BLOSUM90\"])\n",
    "    embedding_size = trial.suggest_int(\"embedding_size\", 5, 40)\n",
    "    conv_kernel_size = trial.suggest_int(\"conv_kernel_size\", 2, patch_size, log=True)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 5e-3)\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", list(Trainer._optimizers_map.keys()))\n",
    "    activation = trial.suggest_categorical(\"activation\", list(Trainer._activations_map.keys()))\n",
    "    # dropout_rate = trial.suggest_float(\"dropout_rate\", 0.05, 0.6)\n",
    "    # antipos_proba = trial.suggest_float(\"antipos_proba\", 0.05, 0.2)\n",
    "    # pos_proba = trial.suggest_float(\"pos_proba\", 0.05, 0.2)\n",
    "    # replacement_proba_factor = trial.suggest_int(\"replacement_proba_factor\", 1, 1000)\n",
    "    \n",
    "    suggested_parameters = dict(patch_size=patch_size, patch_stride=patch_stride,\n",
    "                                substitution_matrix=substitution_matrix, embedding_size=embedding_size,\n",
    "                                conv_kernel_size=conv_kernel_size, lr=lr,\n",
    "                                optimizer=optimizer, activation=activation)\n",
    "    logger.info(f\"Suggested parameters: {suggested_parameters}\")\n",
    "    \n",
    "    mp = mp_preset(\n",
    "        embedding_size=embedding_size,\n",
    "        conv_kernel_size=conv_kernel_size,\n",
    "        activation=activation,\n",
    "        # dropout_rate=dropout_rate,\n",
    "    )\n",
    "\n",
    "    hp = hp_preset(\n",
    "        substitution_matrix=substitution_matrix,\n",
    "        patch_size=patch_size,\n",
    "        optimizer=optimizer,\n",
    "        lr=lr,\n",
    "        patch_stride=patch_stride,\n",
    "        # antipos_proba=antipos_proba,\n",
    "        # pos_proba=pos_proba,\n",
    "        # replacement_proba_factor=replacement_proba_factor,\n",
    "        model_parameters=mp,\n",
    "    )\n",
    "\n",
    "    crossval_metrics = defaultdict(list)\n",
    "    for current_split, (train_indices, val_indices) in enumerate(kfold.split(X, y)):\n",
    "        logger.info(f\"Running nested CV, outer split {outer_split[0]}/{outer_split[1]}, Training inner split {current_split+1}/{kfold.n_splits}\")\n",
    "        logger.debug(f\"Performing kfold split. Train split sample: {train_indices[0:31:2]}, Val split sample: {val_indices[0:31:2]}\")\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "        trainer = trainer_preset(X_train=X_train, X_val=X_val,\n",
    "                                 y_train=y_train, y_val=y_val,\n",
    "                                 hyperparameters=deepcopy(hp))\n",
    "        if use_tensorboard:\n",
    "            inner_split_tb_run = tb_run(f\"nested_CV/outer_split_{outer_split[0]}/trial_{trial.number}/inner_split_{current_split+1}\")\n",
    "            reinit_tensorboard_local(inner_split_tb_run, clear_log=True)\n",
    "            writer = SummaryWriter(log_dir=inner_split_tb_run)\n",
    "        else:\n",
    "            writer = None\n",
    "        loss, metrics = trainer.train(n_epochs=30, valid=3, writer=writer)\n",
    "        for metric, value in metrics.items():\n",
    "            crossval_metrics[metric].append(value)\n",
    "    \n",
    "    crossval_metrics_mean = {metric: sum(values) / len(values)\n",
    "                             for metric, values in crossval_metrics.items()}\n",
    "    if use_tensorboard:\n",
    "        hp_logdir = tb_run(f\"nested_CV/outer_split_{outer_split[0]}/trial_{trial.number}/hyperparameters\")\n",
    "        reinit_tensorboard_local(hp_logdir, clear_log=True)\n",
    "        writer = SummaryWriter(log_dir=hp_logdir)\n",
    "        writer.add_hparams(suggested_parameters, crossval_metrics_mean)\n",
    "        writer.flush()\n",
    "\n",
    "    return -crossval_metrics_mean[\"precision_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPVt3Vjd_2fh"
   },
   "outputs": [],
   "source": [
    "raw_X = np.array(pro_apidaecins_sequences + other_proteins_sequences + antipos_sequences)\n",
    "pos_labels = [1] * len(pro_apidaecins_sequences)\n",
    "neg_labels = [0] * len(other_proteins_sequences) + [2] * len(antipos_sequences)\n",
    "raw_y = np.array(pos_labels + neg_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup fixed parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are specified fixed parameters that shall not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhFWZ0T26VzQ"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "\n",
    "mp_preset = partial(\n",
    "    ModelParameters,\n",
    "    conv_channels=64,\n",
    "    blstm_output_size=128,\n",
    "    lstm_output_size=128,\n",
    ")\n",
    "\n",
    "hp_preset = partial(\n",
    "    Hyperparameters,\n",
    "    device=DEVICE,\n",
    "    encoder=OneHotEncoder(device=DEVICE),\n",
    "    metric_fns=(precision_score, recall_score)\n",
    ")\n",
    "\n",
    "trainer_preset = partial(\n",
    "    Trainer,\n",
    "    model_class=HybridModel,\n",
    "    setup=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifFuzc7QQ_KJ",
    "outputId": "cf0dc983-bc28-4c2c-86d1-bf9aed1cd2cf"
   },
   "outputs": [],
   "source": [
    "n_trials = 30\n",
    "outer_k = 5\n",
    "inner_k = 2\n",
    "print(f\"Total learning rounds: {n_trials * outer_k * inner_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run nested cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7QVCMOWqzkL",
    "outputId": "b068c681-528c-41fe-fd2f-32d29a0e3ab8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nested_cv_results = nested_cv(\n",
    "    trainer_preset=trainer_preset,\n",
    "    objective=objective,\n",
    "    X=raw_X,\n",
    "    y=raw_y,\n",
    "    hp_preset=hp_preset,\n",
    "    mp_preset=mp_preset,\n",
    "    n_trials=n_trials,\n",
    "    outer_k=outer_k,\n",
    "    inner_k=inner_k,\n",
    "    random_state=42,\n",
    "    use_tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save nested cv results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"cv_results\", \"nested_cv_results.pk\"), \"wb\") as file:\n",
    "    pickle.dump(nested_cv_results, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load nested cv results from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"cv_results\", \"nested_cv_results.pk\"), \"rb\") as file:\n",
    "    loaded_cv_results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look on cv results\n",
    "for idx, results in enumerate(loaded_cv_results):\n",
    "    print(f\"Outer split {idx+1}\")\n",
    "    metrics_format = \"\\t\".join([f\"{metric}={value:.4f}\" for metric, value in results.metrics.items()])\n",
    "    hp_format = \"\\t\".join([f\"{parameter}={value}\" for parameter, value in results.best_params.items()])\n",
    "    print(f\"Validation metrics: {metrics_format}\")\n",
    "    print(f\"Best hyperparameters: {hp_format}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in loaded_cv_results:\n",
    "    print(result.best_params, result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt97NU8sN3bI"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize latent representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build 2 components UMAP images of latent data representation (before FC layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yj8rUaHkdCw"
   },
   "outputs": [],
   "source": [
    "# Here you can set up adjustable training and model parameters \n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "mp = ModelParameters(\n",
    "    n_classes=2,\n",
    "    embedding_size=20,\n",
    "    conv_channels=64,\n",
    "    conv_kernel_size=13,\n",
    "    dropout_rate=0.2,\n",
    "\n",
    "    blstm_output_size=128,\n",
    "    lstm_output_size=128,\n",
    "\n",
    "    activation=\"relu\"\n",
    ")\n",
    "\n",
    "hp = Hyperparameters(\n",
    "        device=DEVICE,\n",
    "        batch_size=1000,\n",
    "        patch_size=50,\n",
    "        patch_stride = 1,\n",
    "        substitution_matrix=\"BLOSUM45\",\n",
    "        replacement_proba_factor=250,\n",
    "        pos_proba=0.1,\n",
    "        antipos_proba=0.1,\n",
    "\n",
    "        model_parameters=mp,\n",
    "        encoder=OneHotEncoder(alphabet=\"prot\", device=DEVICE),\n",
    "\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        optimizer=\"adam\",\n",
    "        scheduler=None,\n",
    "        lr=1e-4,\n",
    "\n",
    "        metric_fns=(matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0xV7oyaLYj3"
   },
   "outputs": [],
   "source": [
    "raw_X = np.array(pro_apidaecins_sequences + other_proteins_sequences + antipos_sequences)\n",
    "pos_labels = [1] * len(pro_apidaecins_sequences)\n",
    "neg_labels = [0] * len(other_proteins_sequences) + [2] * len(antipos_sequences)\n",
    "raw_y = np.array(pos_labels + neg_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_X, raw_y, random_state=42, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(HybridModel, X_train, X_test,\n",
    "                  y_train, y_test, hp, setup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗`cache_embeddings` option is very memory consuming, especially if you use SimpleSNN model❗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGZRwmekUFAR",
    "outputId": "eea7823d-2ce9-48f2-c2e5-743c97baea42",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train(n_epochs=100, valid=1, writer=None, vis_seqs=None, cache_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_history = {}\n",
    "for epoch, (embeddings, labels) in tqdm(trainer.embedding_cache.items()):\n",
    "    um = umap.UMAP()\n",
    "    reduced_embedding = um.fit_transform(embeddings)\n",
    "    embedding_history[epoch - 1] = (reduced_embedding, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create UMAP images and save them to directory `images` in alphabetic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"images\" in os.listdir():\n",
    "    shutil.rmtree(\"images\")\n",
    "else:\n",
    "    os.mkdir(\"images\")\n",
    "\n",
    "plt.rc(\"figure\", figsize=(15, 15))\n",
    "\n",
    "suffixes = product(ascii_lowercase, repeat=2)\n",
    "\n",
    "for epoch in sorted(list(embedding_history.keys())):\n",
    "    embeddings, labels = embedding_history[epoch]\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        if label:\n",
    "            new_labels.append(\"Apidaecins\")\n",
    "        else:\n",
    "            new_labels.append(\"Other proteins\")\n",
    "    plt.figure()\n",
    "    sns.scatterplot(embeddings[:, 0], embeddings[:, 1], hue=new_labels)\n",
    "    plt.title(f\"Epoch {epoch}\")\n",
    "    plt.axis(\"off\")\n",
    "    suffix = \"\".join(next(suffixes))\n",
    "    plt.savefig(os.path.join(\"images\", f\"HybridModel_embeddings_{suffix}.png\"), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the best hyperparameters selected on nested cross validation with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "mp = ModelParameters(\n",
    "    n_classes=2,\n",
    "    embedding_size=39,\n",
    "    conv_channels=128,\n",
    "    conv_kernel_size=12,\n",
    "    dropout_rate=0.2,\n",
    "\n",
    "    blstm_output_size=256,\n",
    "    lstm_output_size=256,\n",
    "\n",
    "    activation=\"relu\"\n",
    ")\n",
    "\n",
    "hp = Hyperparameters(\n",
    "        device=DEVICE,\n",
    "        batch_size=1000,\n",
    "        patch_size=33,\n",
    "        patch_stride=1,\n",
    "        substitution_matrix=\"BLOSUM45\",\n",
    "        replacement_proba_factor=250,\n",
    "        pos_proba=0.1,\n",
    "        antipos_proba=0.1,\n",
    "\n",
    "        model_parameters=mp,\n",
    "        encoder=OneHotEncoder(alphabet=\"prot\", device=DEVICE),\n",
    "\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        optimizer=\"adam\",\n",
    "        scheduler=None,\n",
    "        lr=0.0047474,\n",
    "\n",
    "        metric_fns=()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0xV7oyaLYj3"
   },
   "outputs": [],
   "source": [
    "raw_X = np.array(pro_apidaecins_sequences + other_proteins_sequences + antipos_sequences)\n",
    "pos_labels = [1] * len(pro_apidaecins_sequences)\n",
    "neg_labels = [0] * len(other_proteins_sequences) + [2] * len(antipos_sequences)\n",
    "raw_y = np.array(pos_labels + neg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(HybridModel, X_train=raw_X, X_val=None,\n",
    "                  y_train=raw_y, y_val=None, hyperparameters=hp, setup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_name = tb_run(\"Final_training\")\n",
    "reinit_tensorboard_local(run_name, clear_log=True)\n",
    "writer = SummaryWriter(log_dir=run_name)\n",
    "\n",
    "trainer.train(n_epochs=100, writer=writer, valid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model weights and hyperparameters to files. `models` directory contains 2 subdirectories: `weights` and `params`. `weights` contain model weights saved by pytorch, all files have versions. `params` contain saved hyperparameters of model, files also have versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will produce `models/weights/HybridModel_vX.pt` and `models/params/HybridModel_vX.pk`\n",
    "trainer.save_model(\"models\", \"HybridModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor is instance of Trainer class with pretrained model\n",
    "# It uses the most recent version of model found in `models` directory\n",
    "predictor = Trainer.make_predictor(\"models\", \"HybridModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave one-out proteome validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_species = [\n",
    "    'Vespa_mandarinia',\n",
    "    'Bombus_pyrosoma',\n",
    "    'Megalopta_genalis',\n",
    "    'Leptopilina_heterotoma',\n",
    "    'Apis_mellifera_caucasica'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_pro_apidaecins = {}\n",
    "for species in test_species:\n",
    "    parser = SeqIO.parse(PRO_APIDAECINS_PATH, \"fasta\")\n",
    "    species_api_records = filter(lambda rec: species.replace(\"_\", \" \") in rec.description, parser)\n",
    "    species_pro_apidaecins[species] = {record.id: str(record.seq) for record in species_api_records}\n",
    "    print(species, \"has\", len(species_pro_apidaecins[species]), \"selected apidaecins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_apidaecins = {}\n",
    "for species in test_species:\n",
    "    parser = SeqIO.parse(APIDAECINS_PATH, \"fasta\")\n",
    "    species_api_records = filter(lambda rec: species.replace(\"_\", \" \") in rec.description, parser)\n",
    "    species_apidaecins[species] = {record.id: str(record.seq) for record in species_api_records}\n",
    "    print(species, \"has\", len(species_apidaecins[species]), \"unfiltered apidaecins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load true labels for proteomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_masks = defaultdict(dict)\n",
    "\n",
    "for species, species_api_records_dict in species_pro_apidaecins.items():\n",
    "    proteome_records = list(SeqIO.parse(os.path.join(\"data\", \"proteomes\", f\"{species}.faa\"), \"fasta\"))\n",
    "    for record in proteome_records:\n",
    "        if record.id in species_api_records_dict:\n",
    "            match = re.search(species_api_records_dict[record.id], str(record.seq))\n",
    "            true_masks[species][record.id] = [0] * len(record)\n",
    "            true_masks[species][record.id][match.start():match.end()] = [1] * (match.end() - match.start())\n",
    "        else:\n",
    "            true_masks[species][record.id] = [0] * len(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_masks_unfiltered = defaultdict(dict)\n",
    "\n",
    "for species, species_api_records_dict in species_apidaecins.items():\n",
    "    proteome_records = list(SeqIO.parse(os.path.join(\"data\", \"proteomes\", f\"{species}.faa\"), \"fasta\"))\n",
    "    for record in proteome_records:\n",
    "        if record.id in species_api_records_dict:\n",
    "            match = re.search(species_api_records_dict[record.id], str(record.seq))\n",
    "            true_masks_unfiltered[species][record.id] = [0] * len(record)\n",
    "            true_masks_unfiltered[species][record.id][match.start():match.end()] = [1] * (match.end() - match.start())\n",
    "        else:\n",
    "            true_masks_unfiltered[species][record.id] = [0] * len(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "mp = ModelParameters(\n",
    "    n_classes=2,\n",
    "    embedding_size=39,\n",
    "    conv_channels=128,\n",
    "    conv_kernel_size=12,\n",
    "    dropout_rate=0.2,\n",
    "\n",
    "    blstm_output_size=256,\n",
    "    lstm_output_size=256,\n",
    "\n",
    "    activation=\"relu\"\n",
    ")\n",
    "\n",
    "hp = Hyperparameters(\n",
    "        device=DEVICE,\n",
    "        batch_size=1000,\n",
    "        patch_size=33,\n",
    "        patch_stride=1,\n",
    "        substitution_matrix=\"BLOSUM45\",\n",
    "        replacement_proba_factor=250,\n",
    "        pos_proba=0.1,\n",
    "        antipos_proba=0.1,\n",
    "\n",
    "        model_parameters=mp,\n",
    "        encoder=OneHotEncoder(alphabet=\"prot\", device=DEVICE),\n",
    "\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        optimizer=\"adam\",\n",
    "        scheduler=None,\n",
    "        lr=0.0015,\n",
    "\n",
    "        metric_fns=()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_masks = defaultdict(dict)\n",
    "\n",
    "for species in test_species:\n",
    "    # Load data\n",
    "    train_pro_apidaecins_records = filter(lambda rec: species not in rec.description, SeqIO.parse(PRO_APIDAECINS_PATH, \"fasta\"))\n",
    "    train_pro_apidaecins_sequences = list(map(lambda rec: str(rec.seq), train_pro_apidaecins_records))\n",
    "    \n",
    "    # Make training set\n",
    "    raw_X = np.array(train_pro_apidaecins_sequences + other_proteins_sequences + antipos_sequences)\n",
    "    pos_labels = [1] * len(train_pro_apidaecins_sequences)\n",
    "    neg_labels = [0] * len(other_proteins_sequences) + [2] * len(antipos_sequences)\n",
    "    raw_y = np.array(pos_labels + neg_labels)\n",
    "    \n",
    "    # Train model\n",
    "    trainer = Trainer(HybridModel, X_train=raw_X, X_val=None,\n",
    "                      y_train=raw_y, y_val=None, hyperparameters=hp, setup=True)\n",
    "    trainer.train(n_epochs=100, valid=False)\n",
    "    \n",
    "    # ====================== Validation ==============================================================\n",
    "    proteome_records = list(SeqIO.parse(os.path.join(\"data\", \"proteomes\", f\"{species}.faa\"), \"fasta\"))\n",
    "    \n",
    "    for record in tqdm(proteome_records):\n",
    "        mask = trainer.predict_mask(str(record.seq), stride=50)\n",
    "        mask_sum = mask.sum()\n",
    "        if mask_sum:\n",
    "            mask = trainer.predict_mask(str(record.seq), stride=1).tolist()\n",
    "        else:\n",
    "            mask = [0] * len(record)\n",
    "            \n",
    "        predicted_masks[species][record.id] = mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive predictions fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_fractions = []\n",
    "n_proteins = 0\n",
    "for species, masks in predicted_masks.items():\n",
    "    for recid, mask in masks.items():\n",
    "        mask_frac = sum(mask) / len(mask)\n",
    "        if mask_frac:\n",
    "            predicted_fractions.append(mask_frac)\n",
    "        n_proteins += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", figsize=(15, 15))\n",
    "plt.rc(\"font\", size=22)\n",
    "sns.histplot(predicted_fractions)\n",
    "plt.title(f\"Total proteins: {n_proteins}\")\n",
    "_ = plt.xticks(np.arange(0, 1, 0.1))\n",
    "plt.xlabel(\"Positive predctions fraction per protein\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate overall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species, pred_masks in predicted_masks.items():\n",
    "    true_species_mask = []\n",
    "    pred_species_mask = []\n",
    "    for recid, mask in pred_masks.items():\n",
    "        true_species_mask += true_masks[species][recid]\n",
    "        pred_species_mask += predicted_masks[species][recid]\n",
    "    print(f\"Overall metrics for {species}: \", end=\"\")\n",
    "    print(f\"precision={precision_score(true_species_mask, pred_species_mask):.2f}, \", end=\"\")\n",
    "    print(f\"recall={recall_score(true_species_mask, pred_species_mask):.2f}, \", end=\"\")\n",
    "    print(f\"f1={f1_score(true_species_mask, pred_species_mask):.2f}, \", end=\"\")\n",
    "    print(f\"accuracy={accuracy_score(true_species_mask, pred_species_mask):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics on filtered predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species, pred_masks in predicted_masks.items():\n",
    "    true_species_mask = []\n",
    "    pred_species_mask = []\n",
    "    for recid, mask in pred_masks.items():\n",
    "        if sum(mask) / len(mask) > 0.5:\n",
    "            true_species_mask += true_masks_unfiltered[species][recid]\n",
    "            pred_species_mask += predicted_masks[species][recid]\n",
    "    print(f\"Metrics after filtering for {species}: \", end=\"\")\n",
    "    print(f\"precision={precision_score(true_species_mask, pred_species_mask):.2f}, \", end=\"\")\n",
    "    print(f\"recall={recall_score(true_species_mask, pred_species_mask):.2f}, \", end=\"\")\n",
    "    print(f\"f1={f1_score(true_species_mask, pred_species_mask):.2f}, \", end=\"\")\n",
    "    print(f\"accuracy={accuracy_score(true_species_mask, pred_species_mask):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **recall** metric may be inacurate, because true labels are assigned to whole protein (including signal peptides, which are nnot participating in training)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of AMP_search_with_deep_learning_local.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "apidaecins",
   "language": "python",
   "name": "apidaecins"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
